# ğŸ” Remote Job Scraper

Encuentra trabajos remotos "lazy girl jobs" - posiciones entry-level, sin telÃ©fono, bien pagadas.

## ğŸŒ Live Demo

| Servicio | URL |
|----------|-----|
| **Frontend** | https://frontend-three-azure-48.vercel.app |
| **API** | https://remote-job-scraper-production-2b9c.up.railway.app |
| **API Docs** | https://remote-job-scraper-production-2b9c.up.railway.app/docs |

## âœ¨ Features

- ğŸ”„ **Multi-source scraping**: WeWorkRemotely, RemoteOK, Reddit, Indeed, Glassdoor
- ğŸ“ **No-phone filter**: Detecta trabajos que no requieren llamadas
- ğŸ’° **Salary extraction**: Parsea salarios de descripciones
- ğŸ·ï¸ **Auto-categorization**: dev, support, sales, marketing, design, etc.
- ğŸ¯ **Quiz interactivo**: Encuentra tu trabajo ideal
- âš¡ **API REST**: FastAPI con documentaciÃ³n Swagger

## ğŸ“Š Stats Actuales (2026-02-14)

```
ğŸ“¦ Total activos: 1,039 jobs
ğŸš« Sin telÃ©fono: 254
ğŸ’° Con salario: 441
ğŸ‘¨â€ğŸ’» Dev jobs: 195

By Source:
- WeWorkRemotely: 329
- Indeed (JobSpy): 146
- RemoteOK: 98
- Reddit: 88
```

### Ãšltimo scrape

| Fuente | Encontrados | Nuevos |
|--------|-------------|--------|
| RemoteOK | 98 | 5 |
| WeWorkRemotely | 329 | 1 |
| Reddit | 88 | 4 |
| JobSpy (Indeed) | 146 | 64 |
| **Total** | **661** | **74** |

âš ï¸ **Errores conocidos:**
- ZipRecruiter: GDPR blocked (EU)
- Glassdoor: Rate limit 429

## ğŸ› ï¸ Tech Stack

**Backend:**
- Python 3.12
- FastAPI
- SQLite
- JobSpy, BeautifulSoup, PRAW

**Frontend:**
- Next.js 16
- TypeScript
- Tailwind CSS

**Deployment:**
- Railway (API)
- Vercel (Frontend)

## ğŸš€ Local Development

### Backend

```bash
# Install dependencies
pip install -r requirements.txt

# Run scraper
python src/main.py scrape

# Start API
python src/main.py api
# or
uvicorn src.api:app --reload
```

### Frontend

```bash
cd frontend
npm install
npm run dev
```

## ğŸ“¡ API Endpoints

| Endpoint | Description |
|----------|-------------|
| `GET /` | Health check |
| `GET /docs` | Swagger UI |
| `GET /api/stats` | Database statistics |
| `GET /api/jobs` | List jobs with filters |
| `GET /api/jobs/{id}` | Get single job |
| `GET /api/lazy-girl-jobs` | Pre-filtered ideal jobs |

### Query Parameters

- `limit` - Number of results (default: 50)
- `offset` - Pagination offset
- `no_phone` - Filter no-phone jobs (true/false)
- `category` - Filter by category
- `has_salary` - Only jobs with salary info
- `source` - Filter by source
- `search` - Text search in title/company

## ğŸ“ Project Structure

```
remote-job-scraper/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api.py           # FastAPI application
â”‚   â”œâ”€â”€ database.py      # SQLite operations
â”‚   â”œâ”€â”€ main.py          # CLI entrypoint
â”‚   â””â”€â”€ scrapers/        # Job scrapers
â”‚       â”œâ”€â”€ base.py
â”‚       â”œâ”€â”€ remoteok.py
â”‚       â”œâ”€â”€ weworkremotely.py
â”‚       â”œâ”€â”€ reddit.py
â”‚       â””â”€â”€ jobspy_scraper.py
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â””â”€â”€ lib/
â”‚   â””â”€â”€ package.json
â”œâ”€â”€ data/
â”‚   â””â”€â”€ jobs_seed.db     # Seed database
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ railway.toml
â””â”€â”€ README.md
```

## ğŸ”„ Cron Jobs

Actualmente el scraper corre cada 6 horas via **cron local** (Clawdbot gateway).

```bash
0 */6 * * * python src/main.py scrape
```

---

## ğŸ“‹ TODO - Escalabilidad

### ğŸ¯ Problema actual
El cron depende del PC local de Jaume. Si estÃ¡ apagado, no se ejecuta el scraper y los datos se quedan desactualizados.

### âœ… SoluciÃ³n propuesta: GitHub Actions

Migrar el cron a GitHub Actions para que sea independiente:

```yaml
# .github/workflows/scrape.yml
name: Scrape Jobs
on:
  schedule:
    - cron: '0 */6 * * *'  # Cada 6h
  workflow_dispatch:  # Manual trigger

jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'
      - name: Install dependencies
        run: pip install -r requirements.txt
      - name: Run scraper
        run: python src/main.py scrape
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
          REDDIT_CLIENT_ID: ${{ secrets.REDDIT_CLIENT_ID }}
          REDDIT_CLIENT_SECRET: ${{ secrets.REDDIT_CLIENT_SECRET }}
      - name: Upload DB to Railway
        run: # Script para sincronizar DB con Railway
```

### ğŸ“ Pasos para implementar

1. [ ] Crear workflow `.github/workflows/scrape.yml`
2. [ ] Configurar secrets en GitHub (Reddit API, DB URL)
3. [ ] Modificar scraper para escribir directamente a Railway DB (PostgreSQL)
4. [ ] Migrar de SQLite a PostgreSQL en Railway
5. [ ] Desactivar cron local de Clawdbot
6. [ ] Testear ejecuciÃ³n automÃ¡tica

### ğŸ’¡ Alternativas consideradas

| OpciÃ³n | Pros | Contras |
|--------|------|---------|
| **GitHub Actions** â­ | Gratis, ya tiene repo, logs visibles | Necesita sync con DB |
| Railway Cron | Todo en un sitio | Puede costar $ |
| cron-job.org + endpoint | Simple | Timeout en scrapers largos |

---

## ğŸ“ License

MIT

---

Built with ğŸ¦œ by PepLlu & Jaume
