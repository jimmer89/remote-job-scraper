# ğŸ” Remote Job Scraper

Encuentra trabajos remotos "lazy girl jobs" - posiciones entry-level, sin telÃ©fono, bien pagadas.

## ğŸŒ Live Demo

| Servicio | URL |
|----------|-----|
| **Frontend** | https://frontend-three-azure-48.vercel.app |
| **API** | https://remote-job-scraper-production-2b9c.up.railway.app |
| **API Docs** | https://remote-job-scraper-production-2b9c.up.railway.app/docs |

## âœ¨ Features

- ğŸ”„ **Multi-source scraping**: WeWorkRemotely, RemoteOK, Reddit, Indeed, Glassdoor
- ğŸ“ **No-phone filter**: Detecta trabajos que no requieren llamadas
- ğŸ’° **Salary extraction**: Parsea salarios de descripciones
- ğŸ·ï¸ **Auto-categorization**: dev, support, sales, marketing, design, etc.
- ğŸ¯ **Quiz interactivo**: Encuentra tu trabajo ideal
- âš¡ **API REST**: FastAPI con documentaciÃ³n Swagger

## ğŸ“Š Stats Actuales

```
Total Jobs: 669
No-Phone Jobs: 116
Jobs with Salary: 201

By Source:
- WeWorkRemotely: 326
- Indeed: 123
- RemoteOK: 98
- Reddit: 89
- Glassdoor: 33
```

## ğŸ› ï¸ Tech Stack

**Backend:**
- Python 3.12
- FastAPI
- SQLite
- JobSpy, BeautifulSoup, PRAW

**Frontend:**
- Next.js 16
- TypeScript
- Tailwind CSS

**Deployment:**
- Railway (API)
- Vercel (Frontend)

## ğŸš€ Local Development

### Backend

```bash
# Install dependencies
pip install -r requirements.txt

# Run scraper
python src/main.py scrape

# Start API
python src/main.py api
# or
uvicorn src.api:app --reload
```

### Frontend

```bash
cd frontend
npm install
npm run dev
```

## ğŸ“¡ API Endpoints

| Endpoint | Description |
|----------|-------------|
| `GET /` | Health check |
| `GET /docs` | Swagger UI |
| `GET /api/stats` | Database statistics |
| `GET /api/jobs` | List jobs with filters |
| `GET /api/jobs/{id}` | Get single job |
| `GET /api/lazy-girl-jobs` | Pre-filtered ideal jobs |

### Query Parameters

- `limit` - Number of results (default: 50)
- `offset` - Pagination offset
- `no_phone` - Filter no-phone jobs (true/false)
- `category` - Filter by category
- `has_salary` - Only jobs with salary info
- `source` - Filter by source
- `search` - Text search in title/company

## ğŸ“ Project Structure

```
remote-job-scraper/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api.py           # FastAPI application
â”‚   â”œâ”€â”€ database.py      # SQLite operations
â”‚   â”œâ”€â”€ main.py          # CLI entrypoint
â”‚   â””â”€â”€ scrapers/        # Job scrapers
â”‚       â”œâ”€â”€ base.py
â”‚       â”œâ”€â”€ remoteok.py
â”‚       â”œâ”€â”€ weworkremotely.py
â”‚       â”œâ”€â”€ reddit.py
â”‚       â””â”€â”€ jobspy_scraper.py
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â””â”€â”€ lib/
â”‚   â””â”€â”€ package.json
â”œâ”€â”€ data/
â”‚   â””â”€â”€ jobs_seed.db     # Seed database
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ railway.toml
â””â”€â”€ README.md
```

## ğŸ”„ Cron Jobs

El scraper corre automÃ¡ticamente cada 6 horas via cron local:

```bash
0 */6 * * * /path/to/scripts/cron_scrape.sh
```

## ğŸ“ License

MIT

---

Built with ğŸ¦œ by PepLlu & Jaume
